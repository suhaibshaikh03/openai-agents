{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_osSlhnf6Js",
        "outputId": "a29d7f79-77dd-43be-f6b5-3897b1b9a0b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.2/68.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.7/165.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "UhFjRbpyf-yk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "dBaoL98ogBGW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent\n",
        "from agents.run import AgentRunner, set_default_agent_runner, Runner\n",
        "import asyncio\n",
        "\n",
        "class CustomAgentRunner(AgentRunner):\n",
        "  #my own custom run function\n",
        "  async def run(self, starting_agent, input, **kwargs):\n",
        "    print(\"Custom Suhaib's agent runner\")\n",
        "\n",
        "    result = await super().run(starting_agent, input, **kwargs)\n",
        "    return result\n",
        "\n",
        "\n",
        "set_default_agent_runner(CustomAgentRunner())\n",
        "\n",
        "\n",
        "async def main():\n",
        "    math_agent: Agent = Agent(name=\"MathAgent\",\n",
        "                        instructions=\"You are a helpful math assistant.\",\n",
        "                        model=\"gpt-5\")\n",
        "\n",
        "    result = await Runner.run(math_agent, \"why learn math for AI Agents?\")\n",
        "\n",
        "    print(\"\\nCALLING AGENT\\n\")\n",
        "    print(result.final_output)\n",
        "\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADQ-_UxGgBrl",
        "outputId": "02d2fae2-8902-4a1e-c00b-9eeb928d2b76"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Suhaib's agent runner\n",
            "\n",
            "CALLING AGENT\n",
            "\n",
            "Short answer: Math lets you design agents that learn, plan, act, and interact reliably, efficiently, and safely—instead of guessing and tweaking until it works.\n",
            "\n",
            "What math buys you\n",
            "- Clarity: Understand why an algorithm works, when it fails, and how to fix it (not just “which setting on a blog post worked once”).\n",
            "- Efficiency: Choose data structures, optimizers, and approximations that cut compute and memory cost.\n",
            "- Reliability and safety: Quantify uncertainty, prove stability, and get worst‑case or probabilistic guarantees.\n",
            "- Better decisions: Model the world, plan under uncertainty, and trade off exploration vs exploitation.\n",
            "- Transfer and innovation: Read papers, adapt ideas, and create new methods rather than only use canned libraries.\n",
            "\n",
            "Key math areas mapped to agent capabilities\n",
            "- Linear algebra (vectors, matrices, eigen/SVD): Embeddings, transformers, attention, PCA, numerical conditioning.\n",
            "- Calculus and optimization (gradients, convexity, Lagrangians): Training, constrained actions, reward shaping, regularization.\n",
            "- Probability and statistics (Bayes, likelihood, concentration, sampling): Uncertainty, off‑policy evaluation, A/B testing, active learning.\n",
            "- Information theory (entropy, KL, mutual information): Exploration bonuses, variational inference, compression, RL objectives.\n",
            "- Dynamic programming and MDPs (Bellman equations, value iteration): Planning, RL fundamentals, reward shaping (potential‑based).\n",
            "- Control and stability (LQR, Lyapunov, PID): Safe continuous control, disturbance rejection, model‑based policies.\n",
            "- Graphs and search (A*, heuristics, admissibility): Task planning, pathfinding, symbolic‑neural hybrids.\n",
            "- Numerical methods (conditioning, step size, autodiff pitfalls): Stable training and simulation.\n",
            "- Game theory and mechanism design: Multi‑agent coordination, incentives, auctions, adversarial training.\n",
            "- Causality and experimental design: Interventions, counterfactuals, robust policy evaluation.\n",
            "- Stochastic processes and filtering (Markov chains, Kalman/particle filters): Tracking, SLAM, belief state in POMDPs.\n",
            "- Geometry/rigid body math (SE(3), quaternions): Embodied/robotic agents, pose estimation and control.\n",
            "\n",
            "Concrete agent problems where math matters\n",
            "- Off‑policy RL: Importance sampling and variance reduction; when/why doubly robust estimators help.\n",
            "- Reward hacking: Potential‑based shaping preserves optimal policies; others can change them.\n",
            "- Divergence in Q‑learning with function approximation: Understand deadly triad and fixes (target nets, double Q, distributional RL).\n",
            "- Exploration: Design UCB/Thompson bonuses from uncertainty estimates, not arbitrary noise.\n",
            "- Safety/stability: Use Lyapunov functions or control barrier functions to guarantee constraints.\n",
            "- Planning: Prove heuristic admissibility for A* to retain optimality and bound expansions.\n",
            "- Robustness: Use Lipschitz bounds, adversarial training, or certified radii to guarantee behavior under perturbations.\n",
            "\n",
            "How much math for which role\n",
            "- Practitioner/engineer: Linear algebra, calculus, probability, optimization, DP/MDPs, basic statistics and numerics.\n",
            "- Researcher/algorithm designer: All of the above plus measure‑theoretic probability, convex analysis, info theory, control/game theory, and advanced RL theory.\n",
            "- Robotics/embodied agents: Add kinematics/dynamics, estimation, control, and geometric mechanics.\n",
            "\n",
            "A practical learning path\n",
            "- Start from problems you care about (e.g., off‑policy evaluation, motion planning), then learn the math that makes the solution principled.\n",
            "- Implement from scratch: SGD/Adam, value iteration, policy gradients with baselines, A*/D*, Kalman filter. Inspect failures numerically.\n",
            "- Prove small things you use: Convergence of value iteration; admissibility of your heuristic; why baseline doesn’t bias PG; why potential‑based shaping preserves optimality.\n",
            "- Develop numerical instincts: Conditioning, scaling, clipping, line search, mixed precision, Jacobian/Hessian checks.\n",
            "\n",
            "If you share the kind of agents you’re building (web + tools, robotics, multi‑agent sims, etc.), I can tailor a focused math checklist and a few targeted exercises.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "B5eS1DmOg3z3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}